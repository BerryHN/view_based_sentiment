{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练词向量\n",
    "将所有的训练数据、测试数据和网上抓取的汽车评论数据用来训练词向量。首先使用结巴分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import jieba \n",
    "import gensim\n",
    "import logging\n",
    "import multiprocessing\n",
    "from tqdm import tqdm \n",
    "import time \n",
    "import sys\n",
    "import os \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入所有数据，并处理成 [[w1, w2, ...], [sentence2], ...] 的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2017-06-19 16:58:28,565 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2017-06-19 16:58:28,572 : DEBUG : Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.314 seconds.\n",
      "2017-06-19 16:58:28,885 : DEBUG : Loading model cost 0.314 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-06-19 16:58:28,887 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用\n",
      "结巴\n",
      "分词\n",
      "进行\n",
      "分词\n",
      "处理\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "ss = u'使用结巴分词进行分词处理。'\n",
    "for each in jieba.cut(ss):\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 使用多进程实现分词\n",
    "def cut_comment(df):\n",
    "    df['words'] = df['Content'].apply(lambda ss: list(jieba.cut(ss)))\n",
    "    return df\n",
    "\n",
    "def apply_parallel(df_grouped, func):\n",
    "    \"\"\"利用 Parallel 和 delayed 函数实现并行运算\"\"\"\n",
    "    results = Parallel(n_jobs=-1)(delayed(func)(group) for name, group in df_grouped)\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time costed 119.44 seconds\n",
      "time costed 13.0724 seconds\n",
      "There are 73860 sentences\n"
     ]
    }
   ],
   "source": [
    "df_comments = list()  # 把所有的 comments 提取出来。\n",
    "time0 = time.time()\n",
    "# **训练数据和 测试数据\n",
    "raw_data_path = '../raw_data/'\n",
    "raw_files = ['Test.csv','TrainSecond.csv', 'Train.csv', 'TestSecond.csv']\n",
    "for raw_file in raw_files:\n",
    "    file_path = raw_data_path + raw_file\n",
    "    with open(file_path, 'rb') as inp:\n",
    "        df = pd.read_csv(inp, sep='\\t')\n",
    "        df_grouped = df.groupby(df.index)\n",
    "        df = apply_parallel(df_grouped, cut_comment).loc[:, ['Content', 'words']]\n",
    "        df_comments.append(df)\n",
    "print 'time costed %g seconds' % (time.time() - time0)  \n",
    "time0 =time.time()\n",
    "\n",
    "# **抓取的评论数据\n",
    "comment_path = '../comments/'\n",
    "comment_files = os.listdir(comment_path)\n",
    "for comment_file in comment_files:\n",
    "    file_path = comment_path + comment_file\n",
    "    with open(file_path, 'rb') as inp:\n",
    "        df =  pd.read_table(inp, names=['Content'])\n",
    "        df_grouped = df.groupby(df.index)\n",
    "        df = apply_parallel(df_grouped, cut_comment)\n",
    "        df_comments.append(df)\n",
    "        \n",
    "df_comment = pd.concat(df_comments, ignore_index=True)\n",
    "print 'time costed %g seconds' % (time.time() - time0)\n",
    "\n",
    "sentences = df_comment['words'].values\n",
    "print 'There are %d sentences' % len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-19 17:01:11,480 : INFO : collecting all words and their counts\n",
      "2017-06-19 17:01:11,482 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-19 17:01:11,616 : INFO : PROGRESS: at sentence #10000, processed 291863 words, keeping 26020 word types\n",
      "2017-06-19 17:01:11,716 : INFO : PROGRESS: at sentence #20000, processed 585304 words, keeping 37568 word types\n",
      "2017-06-19 17:01:11,810 : INFO : PROGRESS: at sentence #30000, processed 836773 words, keeping 47754 word types\n",
      "2017-06-19 17:01:11,903 : INFO : PROGRESS: at sentence #40000, processed 1122790 words, keeping 55449 word types\n",
      "2017-06-19 17:01:11,999 : INFO : PROGRESS: at sentence #50000, processed 1384733 words, keeping 63274 word types\n",
      "2017-06-19 17:01:12,093 : INFO : PROGRESS: at sentence #60000, processed 1646737 words, keeping 69963 word types\n",
      "2017-06-19 17:01:12,310 : INFO : PROGRESS: at sentence #70000, processed 2469154 words, keeping 82698 word types\n",
      "2017-06-19 17:01:12,656 : INFO : collected 105166 word types from a corpus of 4371490 raw words and 73860 sentences\n",
      "2017-06-19 17:01:12,657 : INFO : Loading a fresh vocabulary\n",
      "2017-06-19 17:01:12,769 : INFO : min_count=5 retains 28484 unique words (27% of original 105166, drops 76682)\n",
      "2017-06-19 17:01:12,769 : INFO : min_count=5 leaves 4249196 word corpus (97% of original 4371490, drops 122294)\n",
      "2017-06-19 17:01:12,854 : INFO : deleting the raw counts dictionary of 105166 items\n",
      "2017-06-19 17:01:12,858 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2017-06-19 17:01:12,859 : INFO : downsampling leaves estimated 3469920 word corpus (81.7% of prior 4249196)\n",
      "2017-06-19 17:01:12,859 : INFO : estimated required memory for 28484 words and 200 dimensions: 59816400 bytes\n",
      "2017-06-19 17:01:12,939 : INFO : resetting layer weights\n",
      "2017-06-19 17:01:13,273 : INFO : training model with 12 workers on 28484 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-06-19 17:01:13,274 : INFO : expecting 73860 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-06-19 17:01:14,299 : INFO : PROGRESS: at 10.36% examples, 836827 words/s, in_qsize 24, out_qsize 1\n",
      "2017-06-19 17:01:15,300 : INFO : PROGRESS: at 18.86% examples, 925870 words/s, in_qsize 22, out_qsize 2\n",
      "2017-06-19 17:01:16,309 : INFO : PROGRESS: at 19.74% examples, 996307 words/s, in_qsize 24, out_qsize 3\n",
      "2017-06-19 17:01:17,318 : INFO : PROGRESS: at 27.34% examples, 1005526 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-19 17:01:18,317 : INFO : PROGRESS: at 38.48% examples, 1002517 words/s, in_qsize 20, out_qsize 3\n",
      "2017-06-19 17:01:19,336 : INFO : PROGRESS: at 39.65% examples, 1029857 words/s, in_qsize 23, out_qsize 2\n",
      "2017-06-19 17:01:20,339 : INFO : PROGRESS: at 43.85% examples, 1029825 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-19 17:01:21,347 : INFO : PROGRESS: at 55.28% examples, 1013020 words/s, in_qsize 24, out_qsize 2\n",
      "2017-06-19 17:01:22,359 : INFO : PROGRESS: at 59.44% examples, 1032907 words/s, in_qsize 24, out_qsize 0\n",
      "2017-06-19 17:01:23,364 : INFO : PROGRESS: at 60.62% examples, 1038407 words/s, in_qsize 24, out_qsize 2\n",
      "2017-06-19 17:01:24,362 : INFO : PROGRESS: at 72.70% examples, 1032555 words/s, in_qsize 24, out_qsize 0\n",
      "2017-06-19 17:01:25,365 : INFO : PROGRESS: at 79.12% examples, 1035526 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-19 17:01:26,374 : INFO : PROGRESS: at 79.84% examples, 1040229 words/s, in_qsize 21, out_qsize 2\n",
      "2017-06-19 17:01:27,396 : INFO : PROGRESS: at 87.95% examples, 1029856 words/s, in_qsize 22, out_qsize 4\n",
      "2017-06-19 17:01:28,398 : INFO : PROGRESS: at 98.56% examples, 1027099 words/s, in_qsize 23, out_qsize 2\n",
      "2017-06-19 17:01:29,395 : INFO : PROGRESS: at 99.65% examples, 1033779 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-19 17:01:29,993 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-06-19 17:01:30,011 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-06-19 17:01:30,022 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-06-19 17:01:30,024 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-06-19 17:01:30,025 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-06-19 17:01:30,026 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-06-19 17:01:30,031 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-06-19 17:01:30,034 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-06-19 17:01:30,041 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-19 17:01:30,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-19 17:01:30,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-19 17:01:30,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-19 17:01:30,050 : INFO : training on 21857450 raw words (17349478 effective words) took 16.8s, 1034694 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 716 ms, total: 1min 16s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "%time model = Word2Vec(sentences=sentences, size=200, window=5, min_count=5, workers=multiprocessing.cpu_count())\n",
    "# model_outp = 'data/pretrained_embedding.model' # 保存训练好的词向量\n",
    "# %time model.save(model_outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出词向量\n",
    "将训练得到的词向量导出为 [vocab_size, embedding_size] 的 np.array 数据。\n",
    "\n",
    "index2word = model.wv.index2word 为一个list，长度为 vocab_size, 每个元素对应一个 vocab.\n",
    "\n",
    "index2vec = model.wv.syn0 也是一个list, 长度为 vocab_size，对应为每个 vocab 的词向量。\n",
    "\n",
    "所以 id = 1, 对应的词为，index2word[id]；对应的词向量为，index2vec[id]\n",
    "\n",
    "由于我们训练好的词向量之后不需要再进行调整，而是直接保存下来在 TensorFlow 中调用。所以直接把 model.wv 保存下来即可，而不必把整个 model 保存下来，这样能够节省更多的内存。model.mv 是 read-only 的，比 model 的体积要小一些。\n",
    "\n",
    "在这例中，数据量比较小。结果 model 保存后为 45.27MB，而 model.wv 保存后占 23.42MB，大概只有前者的一半体积。\n",
    "\n",
    "model 的保存与导入方法是：\n",
    "\n",
    "``` python\n",
    "model.save(file_name)\n",
    "model.load(file_name)\n",
    "```\n",
    "\n",
    "model.wv 的保存于导入方法是：\n",
    "\n",
    "``` python\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(fname)\n",
    "word_vectors = KeyedVectors.load(fname)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-19 17:02:07,858 : INFO : saving KeyedVectors object under data/pretrained_wv.model, separately None\n",
      "2017-06-19 17:02:07,860 : INFO : not storing attribute syn0norm\n",
      "2017-06-19 17:02:08,355 : INFO : saved data/pretrained_wv.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "word_vectors.save('../data/pretrained_wv.model')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "的\n",
      "[-0.09067576 -0.67075008  1.49583769  0.22636585  1.0939517   0.24701905\n",
      "  1.26334977 -1.23074603 -0.17421229 -0.11278294  0.42189774  0.10117231\n",
      " -0.40277553 -0.44207099 -0.20807193 -0.32686305 -0.26631337 -0.11201116\n",
      " -0.53031427  0.16261348]\n"
     ]
    }
   ],
   "source": [
    "index2word = word_vectors.index2word\n",
    "index2vec = word_vectors.syn0\n",
    "id = 1\n",
    "print index2word[id]\n",
    "print index2vec[id][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09067576 -0.67075008  1.49583769  0.22636585  1.0939517   0.24701905\n",
      "  1.26334977 -1.23074603 -0.17421229 -0.11278294  0.42189774  0.10117231\n",
      " -0.40277553 -0.44207099 -0.20807193 -0.32686305 -0.26631337 -0.11201116\n",
      " -0.53031427  0.16261348]\n"
     ]
    }
   ],
   "source": [
    "print word_vectors[u'的'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:225320, index:1, sample_int:670808302)\n"
     ]
    }
   ],
   "source": [
    "# word_vectors.vocab 为一个字典，记录了语料中各个词的信息。\n",
    "# 包括 词频，词的id, 至于 sample_int 什么意思暂时没搞清楚\n",
    "print word_vectors.vocab[u'的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-19 17:02:14,795 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雅阁 0.926233291626\n",
      "世嘉 0.920779943466\n",
      "欧蓝德 0.917513132095\n",
      "冠道 0.913715481758\n",
      "新一代 0.911367356777\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar_cosmul(positive=[u'国产', u'本田'], negative=[u'日本'], topn=5)\n",
    "for w,v in result:\n",
    "    print w,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造 tensorflow 的 embedding \n",
    "参考下面的例子：\n",
    "[Using a pre-trained word embedding (word2vec or Glove) in TensorFlow](https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow)\n",
    "\n",
    "```python\n",
    "W = tf.get_variable(name=\"W\", shape=embedding.shape, tf.constant_initializer(embedding), trainable=False)\n",
    "inputs = tf.nn.embedding_lookup(W, X_inputs) \n",
    "```\n",
    "其中，embedding 就是我们在这里训练得到的词向量：\n",
    "``` python \n",
    "embedding = np.asarray(word_vectors.syn0)\n",
    "```\n",
    "X_input 的每个元素是词所对应的id，和这里的 index2word 所一一对应。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28484, 200)\n",
      "0    ，\n",
      "1    的\n",
      "2    。\n",
      "3    了\n",
      "4    在\n",
      "dtype: object\n",
      "，    0\n",
      "的    1\n",
      "。    2\n",
      "了    3\n",
      "在    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "embedding = np.asanyarray(word_vectors.syn0)\n",
    "print embedding.shape\n",
    "sr_id2word = pd.Series(index2word, index=range(len(index2word)))\n",
    "sr_word2id = pd.Series(range(len(index2word)), index=index2word)\n",
    "\n",
    "print sr_id2word[:5]\n",
    "print sr_word2id[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
