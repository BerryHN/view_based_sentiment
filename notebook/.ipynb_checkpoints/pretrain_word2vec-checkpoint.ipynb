{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练词向量\n",
    "将所有的训练数据、测试数据和网上抓取的汽车评论数据用来训练词向量。首先使用结巴分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import jieba \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from tqdm import tqdm \n",
    "import time \n",
    "import sys\n",
    "import os \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入所有数据，并处理成 [[w1, w2, ...], [sentence2], ...] 的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2017-06-21 16:22:34,401 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2017-06-21 16:22:34,407 : DEBUG : Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.320 seconds.\n",
      "2017-06-21 16:22:34,724 : DEBUG : Loading model cost 0.320 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-06-21 16:22:34,726 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用\n",
      "结巴\n",
      "分词\n",
      "进行\n",
      "分词\n",
      "处理\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "ss = u'使用结巴分词进行分词处理。'\n",
    "for each in jieba.cut(ss):\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 使用多进程实现分词\n",
    "def cut_comment(df):\n",
    "    df['words'] = df['Content'].apply(lambda ss: list(jieba.cut(ss)))\n",
    "    return df\n",
    "\n",
    "def apply_parallel(df_grouped, func):\n",
    "    \"\"\"利用 Parallel 和 delayed 函数实现并行运算\"\"\"\n",
    "    results = Parallel(n_jobs=-1)(delayed(func)(group) for name, group in df_grouped)\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time costed 120.502 seconds\n",
      "time costed 13.4129 seconds\n",
      "There are 73860 sentences\n"
     ]
    }
   ],
   "source": [
    "df_comments = list()  # 把所有的 comments 提取出来。\n",
    "time0 = time.time()\n",
    "# **训练数据和 测试数据\n",
    "raw_data_path = '../raw_data/'\n",
    "raw_files = [ 'Train.csv','TrainSecond.csv', 'Test.csv','TestSecond.csv']\n",
    "for raw_file in raw_files:\n",
    "    file_path = raw_data_path + raw_file\n",
    "    with open(file_path, 'rb') as inp:\n",
    "        df = pd.read_csv(inp, sep='\\t')\n",
    "        df_grouped = df.groupby(df.index)\n",
    "        df = apply_parallel(df_grouped, cut_comment).loc[:, ['Content', 'words']]\n",
    "        df_comments.append(df)\n",
    "print 'time costed %g seconds' % (time.time() - time0)  \n",
    "time0 =time.time()\n",
    "\n",
    "# **抓取的评论数据\n",
    "comment_path = '../comments/'\n",
    "comment_files = os.listdir(comment_path)\n",
    "for comment_file in comment_files:\n",
    "    file_path = comment_path + comment_file\n",
    "    with open(file_path, 'rb') as inp:\n",
    "        df =  pd.read_table(inp, names=['Content'])\n",
    "        df_grouped = df.groupby(df.index)\n",
    "        df = apply_parallel(df_grouped, cut_comment)\n",
    "        df_comments.append(df)\n",
    "        \n",
    "df_comment = pd.concat(df_comments, ignore_index=True)\n",
    "print 'time costed %g seconds' % (time.time() - time0)\n",
    "\n",
    "sentences = df_comment['words'].values\n",
    "print 'There are %d sentences' % len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-21 16:25:59,633 : INFO : collecting all words and their counts\n",
      "2017-06-21 16:25:59,635 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-21 16:25:59,781 : INFO : PROGRESS: at sentence #10000, processed 291104 words, keeping 24897 word types\n",
      "2017-06-21 16:25:59,865 : INFO : PROGRESS: at sentence #20000, processed 554243 words, keeping 38148 word types\n",
      "2017-06-21 16:25:59,967 : INFO : PROGRESS: at sentence #30000, processed 847647 words, keeping 47355 word types\n",
      "2017-06-21 16:26:00,069 : INFO : PROGRESS: at sentence #40000, processed 1122790 words, keeping 55449 word types\n",
      "2017-06-21 16:26:00,164 : INFO : PROGRESS: at sentence #50000, processed 1384733 words, keeping 63274 word types\n",
      "2017-06-21 16:26:00,245 : INFO : PROGRESS: at sentence #60000, processed 1646737 words, keeping 69963 word types\n",
      "2017-06-21 16:26:00,462 : INFO : PROGRESS: at sentence #70000, processed 2469154 words, keeping 82698 word types\n",
      "2017-06-21 16:26:00,826 : INFO : collected 105166 word types from a corpus of 4371490 raw words and 73860 sentences\n",
      "2017-06-21 16:26:00,828 : INFO : Loading a fresh vocabulary\n",
      "2017-06-21 16:26:01,397 : INFO : min_count=1 retains 105166 unique words (100% of original 105166, drops 0)\n",
      "2017-06-21 16:26:01,398 : INFO : min_count=1 leaves 4371490 word corpus (100% of original 4371490, drops 0)\n",
      "2017-06-21 16:26:01,727 : INFO : deleting the raw counts dictionary of 105166 items\n",
      "2017-06-21 16:26:01,731 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2017-06-21 16:26:01,732 : INFO : downsampling leaves estimated 3600984 word corpus (82.4% of prior 4371490)\n",
      "2017-06-21 16:26:01,734 : INFO : estimated required memory for 105166 words and 200 dimensions: 220848600 bytes\n",
      "2017-06-21 16:26:02,058 : INFO : resetting layer weights\n",
      "2017-06-21 16:26:03,495 : INFO : training model with 12 workers on 105166 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-06-21 16:26:03,496 : INFO : expecting 73860 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-06-21 16:26:04,531 : INFO : PROGRESS: at 10.15% examples, 848801 words/s, in_qsize 22, out_qsize 1\n",
      "2017-06-21 16:26:05,548 : INFO : PROGRESS: at 18.79% examples, 921984 words/s, in_qsize 24, out_qsize 2\n",
      "2017-06-21 16:26:06,547 : INFO : PROGRESS: at 19.68% examples, 987839 words/s, in_qsize 23, out_qsize 1\n",
      "2017-06-21 16:26:07,549 : INFO : PROGRESS: at 24.50% examples, 983626 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-21 16:26:08,559 : INFO : PROGRESS: at 35.18% examples, 966878 words/s, in_qsize 24, out_qsize 0\n",
      "2017-06-21 16:26:09,557 : INFO : PROGRESS: at 39.31% examples, 985965 words/s, in_qsize 23, out_qsize 2\n",
      "2017-06-21 16:26:10,578 : INFO : PROGRESS: at 39.91% examples, 1003393 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-21 16:26:11,617 : INFO : PROGRESS: at 50.54% examples, 999898 words/s, in_qsize 22, out_qsize 4\n",
      "2017-06-21 16:26:12,612 : INFO : PROGRESS: at 58.89% examples, 1008519 words/s, in_qsize 24, out_qsize 0\n",
      "2017-06-21 16:26:13,619 : INFO : PROGRESS: at 59.75% examples, 1021942 words/s, in_qsize 24, out_qsize 1\n",
      "2017-06-21 16:26:14,631 : INFO : PROGRESS: at 65.61% examples, 1013587 words/s, in_qsize 24, out_qsize 0\n",
      "2017-06-21 16:26:15,671 : INFO : PROGRESS: at 76.41% examples, 1001800 words/s, in_qsize 23, out_qsize 4\n",
      "2017-06-21 16:26:16,668 : INFO : PROGRESS: at 79.39% examples, 1008328 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-21 16:26:17,669 : INFO : PROGRESS: at 79.91% examples, 1009626 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-21 16:26:18,682 : INFO : PROGRESS: at 89.32% examples, 1001812 words/s, in_qsize 23, out_qsize 0\n",
      "2017-06-21 16:26:19,705 : INFO : PROGRESS: at 98.67% examples, 999574 words/s, in_qsize 23, out_qsize 5\n",
      "2017-06-21 16:26:20,711 : INFO : PROGRESS: at 99.66% examples, 1006802 words/s, in_qsize 24, out_qsize 2\n",
      "2017-06-21 16:26:21,269 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-06-21 16:26:21,277 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-06-21 16:26:21,296 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-06-21 16:26:21,297 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-06-21 16:26:21,299 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-06-21 16:26:21,300 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-06-21 16:26:21,301 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-06-21 16:26:21,302 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-06-21 16:26:21,304 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-21 16:26:21,313 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-21 16:26:21,318 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-21 16:26:21,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-21 16:26:21,319 : INFO : training on 21857450 raw words (18003436 effective words) took 17.8s, 1010594 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 844 ms, total: 1min 23s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%time model = Word2Vec(sentences=sentences, size=200, window=5, min_count=1, workers=multiprocessing.cpu_count())\n",
    "# model_outp = 'data/pretrained_embedding.model' # 保存训练好的词向量\n",
    "# %time model.save(model_outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出词向量\n",
    "将训练得到的词向量导出为 [vocab_size, embedding_size] 的 np.array 数据。\n",
    "\n",
    "index2word = model.wv.index2word 为一个list，长度为 vocab_size, 每个元素对应一个 vocab.\n",
    "\n",
    "index2vec = model.wv.syn0 也是一个list, 长度为 vocab_size，对应为每个 vocab 的词向量。\n",
    "\n",
    "所以 id = 1, 对应的词为，index2word[id]；对应的词向量为，index2vec[id]\n",
    "\n",
    "由于我们训练好的词向量之后不需要再进行调整，而是直接保存下来在 TensorFlow 中调用。所以直接把 model.wv 保存下来即可，而不必把整个 model 保存下来，这样能够节省更多的内存。model.mv 是 read-only 的，比 model 的体积要小一些。\n",
    "\n",
    "在这例中，数据量比较小。结果 model 保存后为 45.27MB，而 model.wv 保存后占 23.42MB，大概只有前者的一半体积。\n",
    "\n",
    "model 的保存与导入方法是：\n",
    "\n",
    "``` python\n",
    "model.save(file_name)\n",
    "model.load(file_name)\n",
    "```\n",
    "\n",
    "model.wv 的保存于导入方法是：\n",
    "\n",
    "``` python\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(fname)\n",
    "word_vectors = KeyedVectors.load(fname)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-21 16:26:40,577 : INFO : saving KeyedVectors object under ../data/pretrained_wv.model, separately None\n",
      "2017-06-21 16:26:40,579 : INFO : not storing attribute syn0norm\n",
      "2017-06-21 16:26:40,580 : INFO : storing np array 'syn0' to ../data/pretrained_wv.model.syn0.npy\n",
      "2017-06-21 16:26:41,953 : INFO : saved ../data/pretrained_wv.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "word_vectors.save('../data/pretrained_wv.model')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "的\n",
      "[-0.78331262 -1.02543843  0.29934859 -0.3891288  -0.18953632  0.49492931\n",
      "  1.70470905 -0.83419657 -0.94910449  0.46337521 -0.23899332 -0.57442439\n",
      " -0.07045357 -1.57719004  0.67850661 -0.02768125  1.06980443  0.64851797\n",
      "  0.11539994  0.59483492]\n"
     ]
    }
   ],
   "source": [
    "index2word = word_vectors.index2word\n",
    "index2vec = word_vectors.syn0\n",
    "id = 1\n",
    "print index2word[id]\n",
    "print index2vec[id][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.78331262 -1.02543843  0.29934859 -0.3891288  -0.18953632  0.49492931\n",
      "  1.70470905 -0.83419657 -0.94910449  0.46337521 -0.23899332 -0.57442439\n",
      " -0.07045357 -1.57719004  0.67850661 -0.02768125  1.06980443  0.64851797\n",
      "  0.11539994  0.59483492]\n"
     ]
    }
   ],
   "source": [
    "print word_vectors[u'的'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:225320, index:1, sample_int:681566758)\n"
     ]
    }
   ],
   "source": [
    "# word_vectors.vocab 为一个字典，记录了语料中各个词的信息。\n",
    "# 包括 词频，词的id, 至于 sample_int 什么意思暂时没搞清楚\n",
    "print word_vectors.vocab[u'的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-21 16:26:46,199 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迈锐宝 0.984410703182\n",
      "雅阁 0.982565581799\n",
      "欧蓝德 0.973016619682\n",
      "途观 0.937127709389\n",
      "睿 0.935886979103\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar_cosmul(positive=[u'国产', u'本田'], negative=[u'日本'], topn=5)\n",
    "for w,v in result:\n",
    "    print w,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造 tensorflow 的 embedding \n",
    "参考下面的例子：\n",
    "[Using a pre-trained word embedding (word2vec or Glove) in TensorFlow](https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow)\n",
    "\n",
    "```python\n",
    "W = tf.get_variable(name=\"W\", shape=embedding.shape, tf.constant_initializer(embedding), trainable=False)\n",
    "inputs = tf.nn.embedding_lookup(W, X_inputs) \n",
    "```\n",
    "其中，embedding 就是我们在这里训练得到的词向量：\n",
    "``` python \n",
    "embedding = np.asarray(word_vectors.syn0)\n",
    "```\n",
    "X_input 的每个元素是词所对应的id，和这里的 index2word 所一一对应。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105166, 200)\n",
      "0    ，\n",
      "1    的\n",
      "2    。\n",
      "3    了\n",
      "4    在\n",
      "dtype: object\n",
      "，    0\n",
      "的    1\n",
      "。    2\n",
      "了    3\n",
      "在    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "embedding = np.asarray(word_vectors.syn0)\n",
    "print embedding.shape\n",
    "sr_id2word = pd.Series(index2word, index=range(len(index2word)))\n",
    "sr_word2id = pd.Series(range(len(index2word)), index=index2word)\n",
    "\n",
    "print sr_id2word[:5]\n",
    "print sr_word2id[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加 UNKNOWN 符号\n",
    "用 句号 的词向量来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "， 0.77500975132\n",
      "； 0.74831867218\n",
      "8.37 0.710772633553\n",
      "一定 0.697985589504\n",
      "同样 0.696451961994\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar_cosmul(u'。', topn=5)\n",
    "for w,v in result:\n",
    "    print w,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把 'UNKNOWN' 符号添加到词向量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pad_word = 'UNKNOWN'\n",
    "vocab_size = index2vec.shape[0]\n",
    "pad_vec = word_vectors[u'。']\n",
    "sr_id2word[vocab_size] = pad_word\n",
    "sr_word2id[pad_word] = vocab_size\n",
    "index2vec = np.vstack([index2vec, pad_vec])\n",
    "W_embedding = index2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "保存数据.\n",
    "W_embedding： shape=[vocab_size, embedding_size] 的 ndarray. 第 i 行表示 id=i 的词的词向量。\n",
    "sr_id2word: \n",
    "sr_word2id:\n",
    "\"\"\"\n",
    "\n",
    "with open('../data/embedding_data.pkl', 'wb') as outp:\n",
    "    pickle.dump(W_embedding, outp)\n",
    "    pickle.dump(sr_id2word, outp)\n",
    "    pickle.dump(sr_word2id, outp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
