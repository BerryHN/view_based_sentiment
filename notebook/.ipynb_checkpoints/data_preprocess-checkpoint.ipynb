{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "将赛方提供的数据处理成为 word/tag 的数据形式。\n",
    "\n",
    "### 多词视角问题：\n",
    "在给出的语料中，有很多视角并不是单个词的形式出现的。为了解决这个问题，我们将视角分成按照单词和多词的不同类型进行标注。\n",
    "\n",
    "- x: 非视角词  \n",
    "- s: 单词视角\n",
    "- b: 视角首词\n",
    "- m: 视角中词\n",
    "- e: 视角尾词\n",
    "\n",
    "把每个词看作一个字符的话，那么每个视角现在就是一个字符串。我们需要根据 Label.csv 提供的结果来把 Train.csv 中的数据转为 word/tag 这种形式的话，相当于在**字符串（句子）中寻找子串（视角）**，并返回子串的位置。这样我们就可以根据子串的长度来给对应位置上的tag赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import jieba \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMP 算法实现子串查找。参考：[字符串匹配的KMP算法](http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def match_len(ss):\n",
    "    \"\"\"获取ss\"前缀\"和\"后缀\"的最长的共有元素的长度。\"\"\"\n",
    "    for i in xrange(1,len(ss)):\n",
    "        if ss[:-i] == ss[i:]:\n",
    "            return len(ss[i:])\n",
    "    return 0\n",
    "\n",
    "def create_map(str_sub):\n",
    "    \"\"\"构建部分匹配表。\"\"\"\n",
    "    str_len = len(str_sub)\n",
    "    match_map = [0] * str_len\n",
    "    for i in xrange(str_len):\n",
    "        match_map[i] = match_len(str_sub[:i+1])\n",
    "    return match_map\n",
    "    \n",
    "    \n",
    "def KMP(list_p, list_sub):\n",
    "    \"\"\"在字符串 list_p 中查找 list_sub, 并返回所有出现的位置。\n",
    "    @list_p: 长度为 n 的字符串。以 list 形式存储。\n",
    "    @list_sub: 长度为 m 的字符串。 以 list 形式存储。\n",
    "    return：\n",
    "    @locs: list,记录 str_sub 在str_p 出现的每个位置（首字符的下标）。\n",
    "    \"\"\"\n",
    "    # 构建部分匹配表，用 match_lens 来存储。\n",
    "    match_map = create_map(list_sub)\n",
    "    locs = list()\n",
    "    n = len(list_p)\n",
    "    m = len(list_sub)\n",
    "    start = 0         # 首字母匹配位置\n",
    "    match_count = 0   # 目前已经匹配的长度\n",
    "    while start <= n-m:  \n",
    "        if match_count == m:  # 若完全匹配\n",
    "            locs.append(start)   # 将当前起始位置返回\n",
    "            start += (m - match_map[m-1])  # 跳越并继续查找\n",
    "            match_count = 0\n",
    "        elif list_p[start+match_count] == list_sub[match_count]:     # 如果当前字符匹配正确\n",
    "            match_count += 1\n",
    "        elif match_count < 2:  # 否则，目前字符匹配不上,跳转\n",
    "            start += 1\n",
    "            match_count = 0\n",
    "        else:\n",
    "            start += (match_count-match_map[match_count-1])\n",
    "            match_count = 0\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\n",
      "[4, 8, 11, 15, 19]\n"
     ]
    }
   ],
   "source": [
    "# 例1  \n",
    "str_p = 'BBC ABCDAB ABCDABCDABDE'\n",
    "str_sub = 'ABCDABD'\n",
    "locs = KMP(str_p, str_sub)\n",
    "print locs\n",
    "\n",
    "# 例2\n",
    "str_p = 'BBC ABCDAB ABCDABCDABDE'\n",
    "str_sub = 'A'\n",
    "locs = KMP(str_p, str_sub)\n",
    "print locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 利用 Label.csv 的结果对 Train.csv 的数据进行标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(tags, locs, view_len):\n",
    "    \"\"\"根据 view_len 给每个词打上 tag.\"\"\"\n",
    "    if view_len == 1:\n",
    "        for loc in locs:\n",
    "            tags[loc] = 's'\n",
    "        return tags\n",
    "    if view_len == 2:\n",
    "        for loc in locs:\n",
    "            tags[loc] = 'b'\n",
    "            tags[loc+1] = 'e'\n",
    "        return tags\n",
    "    for loc in locs:\n",
    "        start = loc\n",
    "        end = loc + view_len - 1\n",
    "        tags[start] = 's'\n",
    "        tags[end] = 'e'\n",
    "        tags[start+1:end] = ['m'] * (view_len-2)\n",
    "    return tags\n",
    "\n",
    "def get_tag(SentenceId):\n",
    "    \"\"\"对每个句子，给每个 word 打上对应的标签。\"\"\"\n",
    "    words  = sr_id2words[SentenceId]\n",
    "    tags = ['x'] * len(words)\n",
    "    if not dict_id2views.has_key(SentenceId): # 说明没有视角\n",
    "        return tags\n",
    "    views = dict_id2views[SentenceId]\n",
    "    for view in views:\n",
    "        locs = KMP(words, view)\n",
    "        if len(locs) == 0:\n",
    "            miss_id.append(SentenceId)\n",
    "            miss_view.append(view)\n",
    "#             print 'Error match %d ' % SentenceId\n",
    "        tags = get_label(tags, locs, len(view))\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.296 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SentenceId                                            Content  \\\n",
      "0       28171  测试的主角---全新速派TSI330智行版测试的主角---全新速派TSI330智行版测试的主...   \n",
      "1       28169  而高尔夫GTI、新速腾GLI和新速腾R-Line这些产品家族中极具动感的车型，也在赛场之外将...   \n",
      "\n",
      "                                               words  \n",
      "0  [测试, 的, 主角, -, -, -, 全新, 速派, TSI330, 智行, 版, 测试...  \n",
      "1  [而, 高尔夫, GTI, 、, 新速腾, GLI, 和, 新速腾, R, -, Line,...  \n",
      "   SentenceId View Opinion  words\n",
      "0       28171   速派     pos   [速派]\n",
      "1       28171  斯柯达     pos  [斯柯达]\n"
     ]
    }
   ],
   "source": [
    "df_Train = pd.read_csv('../raw_data/Train.csv', sep='\\t')\n",
    "df_Label = pd.read_csv('../raw_data/Label.csv', sep='\\t')\n",
    "df_Train['words'] = df_Train.Content.apply(lambda ss: list(jieba.cut(ss)))\n",
    "df_Label['words'] = df_Label.View.apply(lambda ss: list(jieba.cut(ss)))\n",
    "print df_Train.head(2)\n",
    "print df_Label.head(2)\n",
    "\n",
    "# 建立 id:views 对应的列表\n",
    "grouped = df_Label.groupby('SentenceId')\n",
    "# 按照 'SentenceId' 这列分组了，name 为 'SentenceId' 的 key 值，group 为对应的df_group\n",
    "dict_id2views = dict()\n",
    "for SentenceId, group in grouped:\n",
    "    views = group.words.values\n",
    "    dict_id2views[SentenceId] = views\n",
    "sr_id2words = pd.Series(df_Train.words.values, index=df_Train.SentenceId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 304 ms, sys: 0 ns, total: 304 ms\n",
      "Wall time: 300 ms\n",
      "all view num = 10977; miss view num = 781; missing rate = 0.07\n"
     ]
    }
   ],
   "source": [
    "miss_id = list()  # 记录分词错误导致无法匹配的view (id)\n",
    "miss_view = list() \n",
    "%time df_Train['tags'] = df_Train.SentenceId.apply(get_tag)\n",
    "\n",
    "print 'all view num = %d; miss view num = %d; missing rate = %1.2f' % \\\n",
    "       (len(df_Label), len(miss_view), float(len(miss_view)) / len(df_Label))   # 共有 781 个view 没有找到\n",
    "\n",
    "for i in xrange(len(miss_id)):\n",
    "    sentence_id = miss_id[i]\n",
    "#     print 'ID=%s \\n%s\\n' % (sentence_id, ' / '.join(miss_view[i]))\n",
    "#     print ' / '.join(sr_id2words[sentence_id])\n",
    "#     print '-' * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果来分析，大概有 0.07 的视角由于分词的问题导致最后无法正确切分。原因有很多，比如：\n",
    "\n",
    "*新速腾* 结果标注为 *速腾*\n",
    "\n",
    "*/ 捷豹 / 路 / 虎 / *居然连路虎都给分词错误\n",
    "\n",
    "*林肯车 / 型 / *这样的分词错误也是无解了\n",
    " \n",
    " 不过整体来说， 0.07 的错误暂时还能接受。因为不需要刷分数，所以先把这个问题留着，影响不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理 LabelSecond.csv 的结果对 TrainSecond.csv 的数据进行标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SentenceId                       Content  \\\n",
      "0     S30000  天籁公爵——加长版礼宾座驾，以公爵之礼遇，开启专属之享。   \n",
      "1     S30001        比众泰更早一步，江淮版兰博基尼，10万可带走   \n",
      "\n",
      "                                               words  \n",
      "0  [天籁, 公爵, —, —, 加长版, 礼宾, 座, 驾, ，, 以, 公爵, 之, 礼遇,...  \n",
      "1   [比众, 泰, 更, 早, 一步, ，, 江淮, 版, 兰博基尼, ，, 10, 万可, 带走]  \n",
      "  SentenceId  View Opinion     words\n",
      "0     S30000  天籁公爵     neu  [天籁, 公爵]\n",
      "1     S30001    众泰     neu      [众泰]\n"
     ]
    }
   ],
   "source": [
    "df_TrainSecond = pd.read_csv('../raw_data/TrainSecond.csv', sep='\\t')\n",
    "df_LabelSecond = pd.read_csv('../raw_data/LabelSecond.csv', sep='\\t')\n",
    "df_TrainSecond['words'] = df_TrainSecond.Content.apply(lambda ss: list(jieba.cut(ss)))\n",
    "df_LabelSecond['words'] = df_LabelSecond.View.apply(lambda ss: list(jieba.cut(ss)))\n",
    "print df_TrainSecond.head(2)\n",
    "print df_LabelSecond.head(2)\n",
    "\n",
    "# 建立 id:views 对应的列表\n",
    "grouped = df_LabelSecond.groupby('SentenceId')\n",
    "# 按照 'SentenceId' 这列分组了，name 为 'SentenceId' 的 key 值，group 为对应的df_group\n",
    "dict_id2views = dict()\n",
    "for SentenceId, group in grouped:\n",
    "    views = group.words.values\n",
    "    dict_id2views[SentenceId] = views\n",
    "sr_id2words = pd.Series(df_TrainSecond.words.values, index=df_TrainSecond.SentenceId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 244 ms, sys: 12 ms, total: 256 ms\n",
      "Wall time: 238 ms\n",
      "all view num = 10977; miss view num = 900; missing rate = 0.08\n"
     ]
    }
   ],
   "source": [
    "miss_id = list()  # 记录分词错误导致无法匹配的view (id)\n",
    "miss_view = list() \n",
    "%time df_TrainSecond['tags'] = df_TrainSecond.SentenceId.apply(get_tag)\n",
    "\n",
    "print 'all view num = %d; miss view num = %d; missing rate = %1.2f' % \\\n",
    "       (len(df_Label), len(miss_view), float(len(miss_view)) / len(df_Label))   # 共有 781 个view 没有找到\n",
    "\n",
    "for i in xrange(len(miss_id)):\n",
    "    sentence_id = miss_id[i]\n",
    "#     print 'ID=%s \\n%s\\n' % (sentence_id, ' / '.join(miss_view[i]))\n",
    "#     print ' / '.join(sr_id2words[sentence_id])\n",
    "#     print '-' * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 看看最后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28171</td>\n",
       "      <td>[测试, 的, 主角, -, -, -, 全新, 速派, TSI330, 智行, 版, 测试...</td>\n",
       "      <td>[x, x, x, x, x, x, x, s, x, x, x, x, x, x, x, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28169</td>\n",
       "      <td>[而, 高尔夫, GTI, 、, 新速腾, GLI, 和, 新速腾, R, -, Line,...</td>\n",
       "      <td>[x, b, e, x, x, x, x, x, x, x, x, x, x, x, x, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentenceId                                              words  \\\n",
       "0       28171  [测试, 的, 主角, -, -, -, 全新, 速派, TSI330, 智行, 版, 测试...   \n",
       "1       28169  [而, 高尔夫, GTI, 、, 新速腾, GLI, 和, 新速腾, R, -, Line,...   \n",
       "\n",
       "                                                tags  \n",
       "0  [x, x, x, x, x, x, x, s, x, x, x, x, x, x, x, ...  \n",
       "1  [x, b, e, x, x, x, x, x, x, x, x, x, x, x, x, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([df_Train.loc[:, ['SentenceId', 'words', 'tags']],df_Train.loc[:, ['SentenceId', 'words', 'tags']]])\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>View</th>\n",
       "      <th>Opinion</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20591</td>\n",
       "      <td>20591</td>\n",
       "      <td>20591</td>\n",
       "      <td>20591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>14426</td>\n",
       "      <td>3030</td>\n",
       "      <td>3</td>\n",
       "      <td>3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>S36315</td>\n",
       "      <td>大众</td>\n",
       "      <td>neu</td>\n",
       "      <td>[大众]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>21</td>\n",
       "      <td>711</td>\n",
       "      <td>14293</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SentenceId   View Opinion  words\n",
       "count       20591  20591   20591  20591\n",
       "unique      14426   3030       3   3030\n",
       "top        S36315     大众     neu   [大众]\n",
       "freq           21    711   14293    711"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label = pd.concat([df_Label, df_LabelSecond])\n",
    "df_label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 s, sys: 8 ms, total: 1.57 s\n",
      "Wall time: 1.57 s\n",
      "CPU times: user 320 ms, sys: 4 ms, total: 324 ms\n",
      "Wall time: 324 ms\n"
     ]
    }
   ],
   "source": [
    "# 保存结果\n",
    "with open('../data/df_train_label.pkl', 'wb') as outp:\n",
    "    %time pickle.dump(df_train, outp)\n",
    "    %time pickle.dump(df_label, outp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
